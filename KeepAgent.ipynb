{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KeepAgent.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nautiism/i2b2AL/blob/master/KeepAgent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyfQdoMDbHH3",
        "colab_type": "code",
        "outputId": "68931e6f-d750-4161-cc69-7319ad7a5411",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpEp3Ot8Mgf8",
        "colab_type": "code",
        "outputId": "91408e10-f4dc-4bb4-9b2e-592e1c6c6040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#for  uploading the dataset on colab\n",
        "from gensim.models import KeyedVectors\n",
        "filename = 'drive/My Drive'+'/'+'wikipedia-pubmed-and-PMC-w2v.bin'\n",
        "pubmed = KeyedVectors.load_word2vec_format(filename, binary=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWk9FZbJ19Wz",
        "colab_type": "code",
        "outputId": "4a2207d3-5786-45e5-8495-53cda454c868",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch.nn.init as init\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "print(torch.__version__)\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "from sklearn.metrics import euclidean_distances\n",
        "from pyemd import emd\n",
        "import itertools\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import warnings; warnings.simplefilter('ignore')\n"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1.0\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMFYo2QR1DD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weight_init(m):\n",
        "    if isinstance(m, nn.Conv1d):\n",
        "        init.normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.Conv2d):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.Conv3d):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.ConvTranspose1d):\n",
        "        init.normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.ConvTranspose2d):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.ConvTranspose3d):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "            init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.BatchNorm1d):\n",
        "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
        "        init.constant_(m.bias.data, 0)\n",
        "    elif isinstance(m, nn.BatchNorm2d):\n",
        "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
        "        init.constant_(m.bias.data, 0)\n",
        "    elif isinstance(m, nn.BatchNorm3d):\n",
        "        init.normal_(m.weight.data, mean=1, std=0.02)\n",
        "        init.constant_(m.bias.data, 0)\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        init.normal_(m.bias.data)\n",
        "    elif isinstance(m, nn.LSTM):\n",
        "        for param in m.parameters():\n",
        "            if len(param.shape) >= 2:\n",
        "                init.orthogonal_(param.data)\n",
        "            else:\n",
        "                init.normal_(param.data)\n",
        "    elif isinstance(m, nn.LSTMCell):\n",
        "        for param in m.parameters():\n",
        "            if len(param.shape) >= 2:\n",
        "                init.orthogonal_(param.data)\n",
        "            else:\n",
        "                init.normal_(param.data)\n",
        "    elif isinstance(m, nn.GRU):\n",
        "        for param in m.parameters():\n",
        "            if len(param.shape) >= 2:\n",
        "                init.orthogonal_(param.data)\n",
        "            else:\n",
        "                init.normal_(param.data)\n",
        "    elif isinstance(m, nn.GRUCell):\n",
        "        for param in m.parameters():\n",
        "            if len(param.shape) >= 2:\n",
        "                init.orthogonal_(param.data)\n",
        "            else:\n",
        "                init.normal_(param.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GNUxq0g2VzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, hidden_size=150, embedding_length=200): \n",
        "      super(ActorCritic, self).__init__()\n",
        "      \n",
        "      #actor\n",
        "      self.bilstm = nn.LSTM(embedding_length, hidden_size, num_layers=2 , batch_first = True, bidirectional=True)\n",
        "      self.s1 = nn.Linear(2*hidden_size, 150)\n",
        "      self.s2 = nn.Linear(150, 30)\n",
        "      self.fc_layer_1 = nn.Linear(30*2*hidden_size, 1000)\n",
        "      self.fc_layer_2 = nn.Linear(1000, 250)\n",
        "      self.a_compress = nn.Linear(250, 4)\n",
        "      self.c_compress = nn.Linear(250, 1)\n",
        "      \n",
        "      self.logsoft = nn.LogSoftmax(dim=-1)\n",
        "      self.apply(weight_init)\n",
        "      \n",
        "    def attention_net(self, lstm_output,s1,s2):\n",
        "      attn_weight_matrix = s2(torch.tanh(s1(lstm_output)))\n",
        "      attn_weight_matrix = attn_weight_matrix.permute(0, 2, 1)\n",
        "      attn_weight_matrix = F.softmax(attn_weight_matrix, dim=2)\n",
        "      return attn_weight_matrix\n",
        "        \n",
        "    def init(self,h_0,c_0):\n",
        "        self.h_0 = h_0\n",
        "        self.c_0 = c_0\n",
        "    \n",
        "    def forward(self,inp):\n",
        "      output, (h_n, c_n) = self.bilstm(inp, (self.h_0, self.c_0))\n",
        "      attn_weight = self.attention_net(output,self.s1,self.s2)\n",
        "      hidden_matrix = torch.bmm(attn_weight, output)\n",
        "      fc_out = self.fc_layer_1(hidden_matrix.view(-1, hidden_matrix.size()[1]*hidden_matrix.size()[2]))\n",
        "      fc_out = self.fc_layer_2(fc_out)\n",
        "      \n",
        "      a_comp = self.a_compress(fc_out)\n",
        "      sf = self.logsoft(a_comp)\n",
        "      distribution = Categorical(sf)\n",
        "      \n",
        "      value = self.c_compress(fc_out)\n",
        "      return distribution,value\n",
        "      \n",
        "def calculateLoss(rew,logprobs,state_values,gamma=0.99):\n",
        "  rewards = []\n",
        "  dis_reward = 0\n",
        "  for reward in rew[::-1]:\n",
        "      dis_reward = reward + gamma * dis_reward\n",
        "      rewards.insert(0, dis_reward)\n",
        "\n",
        "  rewards = torch.tensor(rewards)\n",
        "  rewards = (rewards - rewards.mean()) / (rewards.std())\n",
        "  rewards = [torch.tensor([reward],requires_grad = True).cuda() for reward in rewards]\n",
        "  logprobs = [torch.tensor([lp],requires_grad = True).cuda() for lp in logprobs]\n",
        "  state_values = [torch.tensor([val],requires_grad = True).cuda() for val in state_values]\n",
        "  \n",
        "  loss = 0\n",
        "  for logprob, value, reward in zip(logprobs, state_values, rewards):\n",
        "      advantage = reward  - value\n",
        "      action_loss = -1*logprob * advantage\n",
        "      value_loss = F.smooth_l1_loss(value, reward)\n",
        "      loss += (action_loss + value_loss)\n",
        "  return loss     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrcZ37rIEota",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "import os\n",
        "name = os.listdir('drive/My Drive/Colab Notebooks/test_data/')\n",
        "tlist = [n.split('.')[0] for n in name if '.txt' in n]\n",
        "\n",
        "tpath = 'drive/My Drive/Colab Notebooks/test_data/'\n",
        "allre = {}\n",
        "for fname in tlist:\n",
        "    f=open(tpath+fname+'.txt')\n",
        "    rec = []\n",
        "    for l in f:\n",
        "        for w in l.split():\n",
        "          rec.append(w.lower())\n",
        "    allre[fname] = rec\n",
        "\n",
        "for rk in allre.keys():\n",
        "    for i,word in enumerate(allre[rk]):\n",
        "      allre[rk][i] = ''.join(e for e in word if e.isalpha())\n",
        "      \n",
        "for rk in allre.keys():\n",
        "  line = []\n",
        "  for word in allre[rk]:\n",
        "    if word not in stop_words and not word.isdigit() and len(word) >= 3:\n",
        "      line.append(word)\n",
        "  allre[rk] = line\n",
        "  \n",
        "tmp = {}\n",
        "for rk in allre.keys():\n",
        "    tmp1 = []\n",
        "    for i,word in enumerate(allre[rk]):\n",
        "      if word in pubmed:\n",
        "        tmp1.append(allre[rk][i])\n",
        "    tmp[rk] = tmp1\n",
        "        \n",
        "sent = tmp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghJhtmnoSrV5",
        "colab_type": "code",
        "outputId": "a2fe679d-e864-4aac-a087-f4b9707e96bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pickle\n",
        "file = open('./drive/My Drive/Colab Notebooks/seedstate.pkl', 'rb')\n",
        "moolstate = pickle.load(file)\n",
        "file.close()\n",
        "moolstate.shape"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 2, 1, 150])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjPonFYWc8V9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Environment\n",
        "import pickle\n",
        "\n",
        "class Env(object):\n",
        "  def __init__(self,sent):\n",
        "    self.sent = sent\n",
        "    self.CBoD = []\n",
        "    self.exit = False\n",
        "    self.loadWMDMatrix()\n",
        "    \n",
        "  def pullList(k):\n",
        "    file = open('./drive/My Drive/Colab Notebooks/wmdm/wmdm_'+k+'.pkl', 'rb')\n",
        "    tmp = pickle.load(file)\n",
        "    file.close()\n",
        "    return tmp\n",
        "    \n",
        "  def loadWMDMatrix(self):\n",
        "    li = [item.split('_')[1].split('.')[0] for item in os.listdir('./drive/My Drive/Colab Notebooks/wmdm/') if 'wmdm_' in item]\n",
        "    self.wmdm = {}\n",
        "    \n",
        "    for k1 in li:\n",
        "      self.wmdm[k1] = pullList(k1)\n",
        "    \n",
        "    for k1 in li:\n",
        "      for k2 in li:\n",
        "        if k1 == k2:\n",
        "          continue\n",
        "        if k2 not in self.wmdm[k1].keys():\n",
        "          self.wmdm[k1][k2] = self.wmdm[k2][k1]\n",
        "        if k1 not in self.wmdm[k2].keys():\n",
        "          self.wmdm[k2][k1] = self.wmdm[k1][k2]\n",
        "    print('Rewards loaded')\n",
        "        \n",
        "  def wmd(self,d1,d2):    \n",
        "   \n",
        "    vect = CountVectorizer(stop_words=\"english\").fit([d1, d2])\n",
        "    v_1, v_2 = vect.transform([d1, d2])\n",
        "    v_1 = v_1.toarray().ravel().astype(np.double)\n",
        "    v_2 = v_2.toarray().ravel().astype(np.double)\n",
        "    v_1 /= v_1.sum()\n",
        "    v_2 /= v_2.sum()\n",
        "    W_ = []\n",
        "    \n",
        "    for w in vect.get_feature_names():\n",
        "      W_.append(pubmed.word_vec(w))\n",
        "     \n",
        "        \n",
        "    D_ = euclidean_distances(W_).astype(np.double)\n",
        "    D_ /= D_.max()\n",
        "    return emd(v_1, v_2, D_)\n",
        "  \n",
        "  def reward(self):\n",
        "    combo = list(itertools.combinations(self.CBoD,2))\n",
        "    sm = 0\n",
        "    \n",
        "    \n",
        "    for d in combo:\n",
        "      try:\n",
        "        sm += self.wmdm[d[0]][d[1]]\n",
        "      except KeyError:\n",
        "        d1 = ' '.join(self.sent[d[0]])\n",
        "        d2 = ' '.join(self.sent[d[1]])\n",
        "        dist_ = self.wmd(d1,d2)\n",
        "        if d[0] not in self.wmdm.keys():\n",
        "          self.wmdm[d[0]] = {}\n",
        "        if d[1] not in self.wmdm.keys():\n",
        "          self.wmdm[d[1]] = {}\n",
        "        self.wmdm[d[0]][d[1]] = dist_\n",
        "        self.wmdm[d[1]][d[0]] = dist_\n",
        "        sm += self.wmdm[d[0]][d[1]]\n",
        "        print('added:'+d[0]+','+d[1])\n",
        "        continue\n",
        "      \n",
        "      \n",
        "    if len(combo)==0:\n",
        "      combo = [1]\n",
        "    \n",
        "    norm_sm = sm/len(combo)\n",
        "    penalty = np.exp(1- 1/len(combo))\n",
        "    if len(combo)==0:\n",
        "      penalty*=20\n",
        "    reward = norm_sm - penalty\n",
        "    return reward\n",
        "  \n",
        "  def makeStateTensor(self):\n",
        "    if len(self.CBoD)==0:\n",
        "      return []\n",
        "    flat_list = []\n",
        "    tmp = [w for k in self.CBoD for w in self.sent[k]]\n",
        "    for w in tmp:\n",
        "        flat_list.append(torch.from_numpy(pubmed.word_vec(w)))\n",
        "    flat_list = torch.stack(flat_list).view(len(flat_list),1,-1)\n",
        "    return flat_list.view(1,-1,200).cuda()\n",
        "   \n",
        "  def step(self,a):\n",
        "    \n",
        "    #act on the action selected by the agent\n",
        "    if a==0: #discard the last sentence \n",
        "      self.CBoD = self.CBoD[:-1]\n",
        "      if len(self.CBoD) == 0:\n",
        "        a=1  \n",
        "    if a==1: #add new sentence\n",
        "      while True:\n",
        "        rkey = np.random.choice(list(self.sent.keys()))\n",
        "        if rkey in self.CBoD:\n",
        "          continue\n",
        "        else:\n",
        "          self.CBoD.append(rkey)\n",
        "          break\n",
        "    if a==2: #end the episode\n",
        "      self.exit = True\n",
        "        \n",
        "    return (self.makeStateTensor(),self.reward(),self.exit)\n",
        "  \n",
        "  def reset(self):\n",
        "    global moolstate\n",
        "    self.CBoD = []\n",
        "    self.exit = False\n",
        "    h_0 = torch.stack([ moolstate[0].cuda() , moolstate[0].cuda() ]).view(4,1,150)\n",
        "    c_0 = torch.stack([ moolstate[1].cuda() , moolstate[1].cuda() ]).view(4,1,150) \n",
        "    rkey = np.random.choice(list(self.sent.keys()))\n",
        "    self.CBoD.append(rkey)\n",
        "    return self.makeStateTensor(),h_0,c_0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40MXGVejbt9s",
        "colab_type": "code",
        "outputId": "9b9ff956-b7f5-4ed3-bfe0-c4b40d46ee05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "\n",
        "def lastMPath():\n",
        "  path = './drive/My Drive/Colab Notebooks/models/'\n",
        "  mods = [f for f in os.listdir(path) if 'model_' in f]\n",
        "  if len(mods) != 0:\n",
        "    return path,max([int(f.split('_')[1]) for f in mods]),True\n",
        "  else:\n",
        "    return '',0,False\n",
        "\n",
        "def train(sent):\n",
        "    render = False\n",
        "    gamma = 0.99\n",
        "    lr = 0.0001\n",
        "    betas = (0.9, 0.999)\n",
        "    random_seed = 543\n",
        "    \n",
        "    torch.manual_seed(random_seed)\n",
        "    \n",
        "    env = Env(sent)\n",
        "    \n",
        "    path,modNo,boole = lastMPath()\n",
        "    ac = None\n",
        "    if boole:\n",
        "      print('Loaded:'+str(modNo))\n",
        "      ac = torch.load(path+'model_'+str(modNo)).cuda()\n",
        "    else:\n",
        "      ac = ActorCritic().cuda()\n",
        "      \n",
        "    opt = optim.Adam(ac.parameters(), lr=lr)\n",
        "    \n",
        "    epi_reward = []\n",
        "    epi_add_discard = []\n",
        "    for i_episode in range(modNo+1, 150000):\n",
        "        running_reward = 0\n",
        "        discard = 0\n",
        "        rewards = []\n",
        "        log_probs = []\n",
        "        values = []\n",
        "        state,h_0,c_0 = env.reset()\n",
        "        ac.init(h_0,c_0)\n",
        "        for t in range(1000):\n",
        "            action_dist,value = ac(state)\n",
        "            action = action_dist.sample()\n",
        "            if action==1:\n",
        "              discard+=1\n",
        "            log_probs.append(action_dist.log_prob(action))\n",
        "            values.append(value)\n",
        "            state, reward, exit = env.step(action)\n",
        "            rewards.append(reward)\n",
        "            running_reward += reward\n",
        "            if exit:\n",
        "                if i_episode % 100 == 0:\n",
        "                  print('Episode:'+str(i_episode))\n",
        "                  print('Saved:'+str(i_episode))\n",
        "                  torch.save(ac, 'drive/My Drive/Colab Notebooks/models/model_'+str(i_episode)+'')\n",
        "                  print('Episode Loss: '+str(running_reward))\n",
        "                  print('Bo Documents: '+str(env.CBoD))\n",
        "                  print('Discards: '+str(discard))\n",
        "                break\n",
        "        opt.zero_grad()\n",
        "        loss = calculateLoss(rewards,log_probs,values,gamma)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        epi_reward.append(running_reward)\n",
        "        epi_add_discard.append(discard)\n",
        "    return ac,epi_reward,epi_add_discard\n",
        "\n",
        "ac,epi_reward,epi_add_discard = train(sent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rewards loaded\n",
            "Episode:100\n",
            "Saved:100\n",
            "Episode Loss: -3.0\n",
            "Bo Documents: ['0437']\n",
            "Discards: 0\n",
            "Episode:200\n",
            "Saved:200\n",
            "Episode Loss: -4.0\n",
            "Bo Documents: ['0226']\n",
            "Discards: 0\n",
            "Episode:300\n",
            "Saved:300\n",
            "Episode Loss: -1.0\n",
            "Bo Documents: ['0424']\n",
            "Discards: 0\n",
            "Episode:400\n",
            "Saved:400\n",
            "Episode Loss: -3.0\n",
            "Bo Documents: ['0118']\n",
            "Discards: 0\n",
            "Episode:500\n",
            "Saved:500\n",
            "Episode Loss: -5.0\n",
            "Bo Documents: ['0246']\n",
            "Discards: 0\n",
            "Episode:600\n",
            "Saved:600\n",
            "Episode Loss: -4.0\n",
            "Bo Documents: ['0301']\n",
            "Discards: 0\n",
            "Episode:700\n",
            "Saved:700\n",
            "Episode Loss: -1.0\n",
            "Bo Documents: ['0265']\n",
            "Discards: 0\n",
            "Episode:800\n",
            "Saved:800\n",
            "Episode Loss: -8.0\n",
            "Bo Documents: ['0113']\n",
            "Discards: 0\n",
            "Episode:900\n",
            "Saved:900\n",
            "Episode Loss: -1.0\n",
            "Bo Documents: ['0190']\n",
            "Discards: 0\n",
            "Episode:1000\n",
            "Saved:1000\n",
            "Episode Loss: -2.0\n",
            "Bo Documents: ['0169']\n",
            "Discards: 0\n",
            "Episode:1100\n",
            "Saved:1100\n",
            "Episode Loss: -4.0\n",
            "Bo Documents: ['0046']\n",
            "Discards: 0\n",
            "Episode:1200\n",
            "Saved:1200\n",
            "Episode Loss: -2.0\n",
            "Bo Documents: ['0050']\n",
            "Discards: 0\n",
            "Episode:1300\n",
            "Saved:1300\n",
            "Episode Loss: -1.0\n",
            "Bo Documents: ['0053']\n",
            "Discards: 0\n",
            "Episode:1400\n",
            "Saved:1400\n",
            "Episode Loss: -5.0\n",
            "Bo Documents: ['0213']\n",
            "Discards: 0\n",
            "Episode:1500\n",
            "Saved:1500\n",
            "Episode Loss: -4.0\n",
            "Bo Documents: ['0258']\n",
            "Discards: 0\n",
            "Episode:1600\n",
            "Saved:1600\n",
            "Episode Loss: -3.0\n",
            "Bo Documents: ['0361']\n",
            "Discards: 0\n",
            "Episode:1700\n",
            "Saved:1700\n",
            "Episode Loss: -2.0\n",
            "Bo Documents: ['0105']\n",
            "Discards: 0\n",
            "Episode:1800\n",
            "Saved:1800\n",
            "Episode Loss: -7.0\n",
            "Bo Documents: ['0322']\n",
            "Discards: 0\n",
            "Episode:1900\n",
            "Saved:1900\n",
            "Episode Loss: -5.0\n",
            "Bo Documents: ['0382']\n",
            "Discards: 0\n",
            "Episode:2000\n",
            "Saved:2000\n",
            "Episode Loss: -1.0\n",
            "Bo Documents: ['0001']\n",
            "Discards: 0\n",
            "Episode:2100\n",
            "Saved:2100\n",
            "Episode Loss: -3.0\n",
            "Bo Documents: ['0194']\n",
            "Discards: 0\n",
            "Episode:2200\n",
            "Saved:2200\n",
            "Episode Loss: -1.0\n",
            "Bo Documents: ['0125']\n",
            "Discards: 0\n",
            "Episode:2300\n",
            "Saved:2300\n",
            "Episode Loss: -2.0\n",
            "Bo Documents: ['0309']\n",
            "Discards: 0\n",
            "Episode:2400\n",
            "Saved:2400\n",
            "Episode Loss: -1.0\n",
            "Bo Documents: ['0126']\n",
            "Discards: 0\n",
            "Episode:2500\n",
            "Saved:2500\n",
            "Episode Loss: -2.0\n",
            "Bo Documents: ['0469']\n",
            "Discards: 0\n",
            "Episode:2600\n",
            "Saved:2600\n",
            "Episode Loss: -1.0\n",
            "Bo Documents: ['0222']\n",
            "Discards: 0\n",
            "Episode:2700\n",
            "Saved:2700\n",
            "Episode Loss: -2.0\n",
            "Bo Documents: ['0265']\n",
            "Discards: 0\n",
            "Episode:2800\n",
            "Saved:2800\n",
            "Episode Loss: -11.0\n",
            "Bo Documents: ['0014']\n",
            "Discards: 0\n",
            "Episode:2900\n",
            "Saved:2900\n",
            "Episode Loss: -1.0\n",
            "Bo Documents: ['0109']\n",
            "Discards: 0\n",
            "Episode:3000\n",
            "Saved:3000\n",
            "Episode Loss: -4.0\n",
            "Bo Documents: ['0401']\n",
            "Discards: 0\n",
            "Episode:3100\n",
            "Saved:3100\n",
            "Episode Loss: -2.0\n",
            "Bo Documents: ['0258']\n",
            "Discards: 0\n",
            "Episode:3200\n",
            "Saved:3200\n",
            "Episode Loss: -2.0\n",
            "Bo Documents: ['0257']\n",
            "Discards: 0\n",
            "Episode:3300\n",
            "Saved:3300\n",
            "Episode Loss: -7.541809219729998\n",
            "Bo Documents: ['0233']\n",
            "Discards: 1\n",
            "Episode:3400\n",
            "Saved:3400\n",
            "Episode Loss: -7.0\n",
            "Bo Documents: ['0085']\n",
            "Discards: 0\n",
            "Episode:3500\n",
            "Saved:3500\n",
            "Episode Loss: -1.0\n",
            "Bo Documents: ['0190']\n",
            "Discards: 0\n",
            "Episode:3600\n",
            "Saved:3600\n",
            "Episode Loss: -1.0\n",
            "Bo Documents: ['0374']\n",
            "Discards: 0\n",
            "Episode:3700\n",
            "Saved:3700\n",
            "Episode Loss: -3.0\n",
            "Bo Documents: ['0154']\n",
            "Discards: 0\n",
            "Episode:3800\n",
            "Saved:3800\n",
            "Episode Loss: -4.0\n",
            "Bo Documents: ['0274']\n",
            "Discards: 0\n",
            "Episode:3900\n",
            "Saved:3900\n",
            "Episode Loss: -3.0\n",
            "Bo Documents: ['0398']\n",
            "Discards: 0\n",
            "Episode:4000\n",
            "Saved:4000\n",
            "Episode Loss: -5.0\n",
            "Bo Documents: ['0357']\n",
            "Discards: 0\n",
            "Episode:4100\n",
            "Saved:4100\n",
            "Episode Loss: -2.0\n",
            "Bo Documents: ['0433']\n",
            "Discards: 0\n",
            "Episode:4200\n",
            "Saved:4200\n",
            "Episode Loss: -5.0\n",
            "Bo Documents: ['0293']\n",
            "Discards: 0\n",
            "Episode:4300\n",
            "Saved:4300\n",
            "Episode Loss: -3.0\n",
            "Bo Documents: ['0105']\n",
            "Discards: 0\n",
            "Episode:4400\n",
            "Saved:4400\n",
            "Episode Loss: -1.0\n",
            "Bo Documents: ['0428']\n",
            "Discards: 0\n",
            "Episode:4500\n",
            "Saved:4500\n",
            "Episode Loss: -4.0\n",
            "Bo Documents: ['0106']\n",
            "Discards: 0\n",
            "Episode:4600\n",
            "Saved:4600\n",
            "Episode Loss: -11.0\n",
            "Bo Documents: ['0443']\n",
            "Discards: 0\n",
            "Episode:4700\n",
            "Saved:4700\n",
            "Episode Loss: -4.0\n",
            "Bo Documents: ['0334']\n",
            "Discards: 0\n",
            "Episode:4800\n",
            "Saved:4800\n",
            "Episode Loss: -2.0\n",
            "Bo Documents: ['0149']\n",
            "Discards: 0\n",
            "Episode:4900\n",
            "Saved:4900\n",
            "Episode Loss: -4.0\n",
            "Bo Documents: ['0416']\n",
            "Discards: 0\n",
            "Episode:5000\n",
            "Saved:5000\n",
            "Episode Loss: -4.0\n",
            "Bo Documents: ['0050']\n",
            "Discards: 0\n",
            "Episode:5100\n",
            "Saved:5100\n",
            "Episode Loss: -5.0\n",
            "Bo Documents: ['0463']\n",
            "Discards: 0\n",
            "Episode:5200\n",
            "Saved:5200\n",
            "Episode Loss: -1.0\n",
            "Bo Documents: ['0467']\n",
            "Discards: 0\n",
            "Episode:5300\n",
            "Saved:5300\n",
            "Episode Loss: -1.0\n",
            "Bo Documents: ['0145']\n",
            "Discards: 0\n",
            "Episode:5400\n",
            "Saved:5400\n",
            "Episode Loss: -4.0\n",
            "Bo Documents: ['0201']\n",
            "Discards: 0\n",
            "Episode:5500\n",
            "Saved:5500\n",
            "Episode Loss: -2.0\n",
            "Bo Documents: ['0290']\n",
            "Discards: 0\n",
            "Episode:5600\n",
            "Saved:5600\n",
            "Episode Loss: -2.0\n",
            "Bo Documents: ['0025']\n",
            "Discards: 0\n",
            "Episode:5700\n",
            "Saved:5700\n",
            "Episode Loss: -1.0\n",
            "Bo Documents: ['0428']\n",
            "Discards: 0\n",
            "Episode:5800\n",
            "Saved:5800\n",
            "Episode Loss: -3.0\n",
            "Bo Documents: ['0413']\n",
            "Discards: 0\n",
            "Episode:5900\n",
            "Saved:5900\n",
            "Episode Loss: -8.0\n",
            "Bo Documents: ['0089']\n",
            "Discards: 0\n",
            "Episode:6000\n",
            "Saved:6000\n",
            "Episode Loss: -11.658830289015999\n",
            "Bo Documents: ['0190']\n",
            "Discards: 1\n",
            "Episode:6100\n",
            "Saved:6100\n",
            "Episode Loss: -1.0\n",
            "Bo Documents: ['0285']\n",
            "Discards: 0\n",
            "Episode:6200\n",
            "Saved:6200\n",
            "Episode Loss: -2.0\n",
            "Bo Documents: ['0253']\n",
            "Discards: 0\n",
            "Episode:6300\n",
            "Saved:6300\n",
            "Episode Loss: -6.0\n",
            "Bo Documents: ['0461']\n",
            "Discards: 0\n",
            "Episode:6400\n",
            "Saved:6400\n",
            "Episode Loss: -1.0\n",
            "Bo Documents: ['0464']\n",
            "Discards: 0\n",
            "Episode:6500\n",
            "Saved:6500\n",
            "Episode Loss: -2.0\n",
            "Bo Documents: ['0201']\n",
            "Discards: 0\n",
            "Episode:6600\n",
            "Saved:6600\n",
            "Episode Loss: -4.41266880281399\n",
            "Bo Documents: ['0137']\n",
            "Discards: 1\n",
            "Episode:6700\n",
            "Saved:6700\n",
            "Episode Loss: -2.0\n",
            "Bo Documents: ['0369']\n",
            "Discards: 0\n",
            "Episode:6800\n",
            "Saved:6800\n",
            "Episode Loss: -1.0\n",
            "Bo Documents: ['0362']\n",
            "Discards: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzyoTVWKkjkr",
        "colab_type": "code",
        "outputId": "39985d59-b131-41aa-a13c-86d10a7a37ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(epi_reward).plot.line()"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe3f1fa1438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEXkEEz4RkUcw4RMRecT/A9CKc767yT9dAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr23Br-hlYsc",
        "colab_type": "code",
        "outputId": "d02169e5-7926-47e7-b772-fd78121850cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "pd.DataFrame(epi_add_discard).plot.line()"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe4007500f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJy6LLQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPkBC4urnKj0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "file = open('drive/My Drive/Colab Notebooks/test_dat.pkl', 'rb')\n",
        "test_dat = pickle.load(file)\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q31JC1bin8vO",
        "colab_type": "code",
        "outputId": "43cd48f2-13e1-4bc5-db51-17186a5177fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "def test(test_dat):\n",
        "  env = Env(test_dat)\n",
        "  combo = []\n",
        "  for i_episode in range(0, 1):\n",
        "      state,h_0,c_0 = env.reset()\n",
        "      ac.init(h_0,c_0)\n",
        "      for t in range(500):\n",
        "          action_dist,value = ac(state)\n",
        "          action = action_dist.sample()\n",
        "          state, reward, exit = env.step(action)\n",
        "          if exit:\n",
        "             combo.append(env.CBoD)\n",
        "  return list(set([item for sublist in combo for item in sublist]))\n",
        "     \n",
        "agent_set = test(test_dat)"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rewards loaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-148-9e974c1dc061>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcombo\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0magent_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-148-9e974c1dc061>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(test_dat)\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0mac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m           \u001b[0maction_dist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m           \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-138-8931a7696346>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m       \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_n\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbilstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m       \u001b[0mattn_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0mhidden_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;31m# type: (Tensor, Optional[Tuple[Tensor, Tensor]]) -> Tuple[Tensor, Tuple[Tensor, Tensor]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m         \u001b[0mmax_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0msorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1r2bud8X48wB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = open('./drive/My Drive/Colab Notebooks/agent_dat.pkl', 'wb')\n",
        "pickle.dump(testing_dat,file)\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}